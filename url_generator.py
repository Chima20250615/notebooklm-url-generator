import streamlit as st
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

st.title("ğŸ“„ NotebookLMç”¨URLä¸€è¦§ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼ï¼ˆMarkdownä»˜ãï¼‰")
st.write("æŒ‡å®šã—ãŸWebã‚µã‚¤ãƒˆå†…ã®ä¸‹å±¤ãƒšãƒ¼ã‚¸URLã‚’ä¸€è¦§åŒ–ã—ã€ã‚¿ã‚¤ãƒˆãƒ«ä»˜ãMarkdownå½¢å¼ã§ã‚‚å‡ºåŠ›ã§ãã¾ã™")

# å…¥åŠ›ï¼šURL
base_url = st.text_input("å¯¾è±¡ã‚µã‚¤ãƒˆã®ãƒˆãƒƒãƒ—URLã‚’å…¥åŠ›ã—ã¦ã­", "https://example.com")

if st.button("URLä¸€è¦§ã‚’å–å¾—ï¼"):
    try:
        response = requests.get(base_url)
        soup = BeautifulSoup(response.text, 'html.parser')

        parsed_base = urlparse(base_url)
        domain = f"{parsed_base.scheme}://{parsed_base.netloc}"

        links = set()
        for tag in soup.find_all("a", href=True):
            href = tag["href"]
            full_url = urljoin(base_url, href)
            if full_url.startswith(domain):
                links.add(full_url)

        if links:
            st.success(f"{len(links)}ä»¶ã®URLã‚’å–å¾—ã—ã¾ã—ãŸğŸ‘‡")

            # é€šå¸¸URLè¡¨ç¤ºï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªç¢ºèªç”¨ï¼‰
            for link in sorted(links):
                st.text(link)

# Markdownå‡ºåŠ›ç”¨
st.markdown("---")
st.subheader("ğŸ“‹ ã‚¿ã‚¤ãƒˆãƒ«ä»˜ãMarkdownå½¢å¼ï¼ˆNotebookLMã«è²¼ã‚Šä»˜ã‘OKï¼‰")
markdown_output = ""

for link in sorted(links):
    try:
        r = requests.get(link, timeout=5)
        r.encoding = r.apparent_encoding  # â† ã“ã“ã§è‡ªå‹•åˆ¤å®š
        inner_soup = BeautifulSoup(r.text, 'html.parser')
        title = inner_soup.title.string.strip() if inner_soup.title else "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãªã—ï¼‰"
    except Exception as e:
        title = "ï¼ˆã‚¿ã‚¤ãƒˆãƒ«å–å¾—å¤±æ•—ï¼‰"

    markdown_output += f"- [{title}]({link})\n"

st.text_area("ğŸ”— ã‚³ãƒ”ãƒ¼ã—ã¦ä½¿ã£ã¦ã­", markdown_output, height=300)

        else:
            st.warning("å†…éƒ¨ãƒªãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    except Exception as e:
        st.error(f"å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
